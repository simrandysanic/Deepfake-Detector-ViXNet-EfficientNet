{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image DeepFake Detection Model Training, Validation, Testing on FF++ Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:**  \n",
    "> Activate the virtual environment before running:  \n",
    "> `conda activate new_env`\n",
    ">\n",
    "> If the environment does not exist, create it using the following commands:\n",
    ">\n",
    "> ```\n",
    "> conda create -n new_env python=3.12 -y\n",
    "> conda activate new_env\n",
    "> ```\n",
    ">\n",
    "> Upgrading/installing necessary libraries/packages\n",
    ">\n",
    "> ```\n",
    "> pip install --upgrade pip\n",
    "> pip install torch torchvision transformers pillow scikit-learn jupyter ipywidgets\n",
    "> ```\n",
    ">\n",
    "> Verifying\n",
    ">\n",
    "> ```\n",
    "> which python\n",
    "> python -c \"import torch; print(torch.__version__)\"\n",
    "> python -c \"from torch.cuda.amp import GradScaler, autocast; print('Imports successful')\"\n",
    "> python -c \"from sklearn.model_selection import train_test_split; print('scikit-learn installed')\"\n",
    "> pip show torch torchvision transformers pillow scikit-learn\n",
    "> ```\n",
    ">\n",
    "> **Select the interpreter from the upper right-hand side (RHS) in Jupyter:**  \n",
    "> `new_env (Python 3.12.9) miniconda3/envs/new_env/bin/python - Conda Env`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 17276, Validation samples: 4319, Test samples: 4200\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for dataset handling\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define dataset paths in the ghulam container\n",
    "base_dir = \"/home/ghulam/FF++/cropped_face_mtcnn\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "# Collect training images from nested real/fake subdirectories\n",
    "real_train_images = [os.path.join(train_dir, \"real\", subdir, img) \n",
    "                    for subdir in os.listdir(os.path.join(train_dir, \"real\")) \n",
    "                    for img in os.listdir(os.path.join(train_dir, \"real\", subdir))]\n",
    "fake_train_images = [os.path.join(train_dir, \"fake\", subdir, img) \n",
    "                    for subdir in os.listdir(os.path.join(train_dir, \"fake\")) \n",
    "                    for img in os.listdir(os.path.join(train_dir, \"fake\", subdir))]\n",
    "\n",
    "# Combine images and assign labels (0 for real, 1 for fake)\n",
    "train_image_paths = real_train_images + fake_train_images\n",
    "train_labels = [0] * len(real_train_images) + [1] * len(fake_train_images)\n",
    "\n",
    "# Split training data into train and validation\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    train_image_paths, train_labels, test_size=0.2, stratify=train_labels\n",
    ")\n",
    "\n",
    "# Collect test images\n",
    "real_test_images = [os.path.join(test_dir, \"real\", subdir, img) \n",
    "                   for subdir in os.listdir(os.path.join(test_dir, \"real\")) \n",
    "                   for img in os.listdir(os.path.join(test_dir, \"real\", subdir))]\n",
    "fake_test_images = [os.path.join(test_dir, \"fake\", subdir, img) \n",
    "                   for subdir in os.listdir(os.path.join(test_dir, \"fake\")) \n",
    "                   for img in os.listdir(os.path.join(test_dir, \"fake\", subdir))]\n",
    "\n",
    "test_paths = real_test_images + fake_test_images\n",
    "test_labels = [0] * len(real_test_images) + [1] * len(fake_test_images)\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)}, Validation samples: {len(val_paths)}, Test samples: {len(test_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for dataset creation, image processing, and PyTorch functionality\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define image transformations to preprocess FF++ dataset\n",
    "transform = transforms.Compose([\n",
    "    # Resize images to 224x224 pixels for model compatibility\n",
    "    transforms.Resize((224, 224)),\n",
    "    # Convert images to PyTorch tensors\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize pixel values using ImageNet mean and standard deviation\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define custom dataset class for FF++ images with error handling\n",
    "class DeepfakeDataset(Dataset):\n",
    "    # Initialize with image paths, labels, and transformation pipeline\n",
    "    def __init__(self, image_paths, labels, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.valid_indices = []\n",
    "        # Specify log file to record invalid images\n",
    "        log_file = \"/home/ghulam/FF++/invalid_images.txt\"\n",
    "        # Open log file in write mode\n",
    "        with open(log_file, \"w\") as log:\n",
    "            # Check each image for validity\n",
    "            for idx, path in enumerate(self.image_paths):\n",
    "                # Skip if the file does not exist\n",
    "                if not os.path.isfile(path):\n",
    "                    print(f\"Skipping missing file: {path}\")\n",
    "                    log.write(f\"{path}: File does not exist\\n\")\n",
    "                    continue\n",
    "                try:\n",
    "                    # Attempt to open and convert image to RGB to verify integrity\n",
    "                    img = Image.open(path).convert(\"RGB\")\n",
    "                    img.close()\n",
    "                    # Store index of valid image\n",
    "                    self.valid_indices.append(idx)\n",
    "                except Exception as e:\n",
    "                    # Log and skip invalid images\n",
    "                    print(f\"Skipping invalid image: {path} ({e})\")\n",
    "                    log.write(f\"{path}: {e}\\n\")\n",
    "        # Report the number of valid images loaded\n",
    "        print(f\"Loaded {len(self.valid_indices)} valid images out of {len(self.image_paths)}\")\n",
    "        # Warn if no valid images are found\n",
    "        if len(self.valid_indices) == 0:\n",
    "            print(f\"Error: No valid images found. Check {log_file} for details.\")\n",
    "\n",
    "    # Return the number of valid images in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    # Retrieve image and label for a given index\n",
    "    def __getitem__(self, idx):\n",
    "        # Map index to valid image index\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        try:\n",
    "            # Load and convert image to RGB\n",
    "            image = Image.open(self.image_paths[actual_idx]).convert(\"RGB\")\n",
    "            # Apply transformations (resize, normalize, etc.)\n",
    "            image = self.transform(image)\n",
    "            # Convert label to PyTorch tensor with long dtype\n",
    "            label = torch.tensor(self.labels[actual_idx], dtype=torch.long)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            # Log errors during image loading and return None\n",
    "            print(f\"Error loading image {self.image_paths[actual_idx]}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17276 valid images out of 17276\n",
      "Loaded 4319 valid images out of 4319\n",
      "Loaded 4200 valid images out of 4200\n",
      "Train loader batches: 4319, Val loader batches: 1080, Test loader batches: 1050\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch DataLoader for batch processing\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataset instances for training, validation, and testing\n",
    "# - Use DeepfakeDataset with pre-defined paths, labels, and transformations\n",
    "train_dataset = DeepfakeDataset(train_paths, train_labels, transform)\n",
    "val_dataset = DeepfakeDataset(val_paths, val_labels, transform)\n",
    "test_dataset = DeepfakeDataset(test_paths, test_labels, transform)\n",
    "\n",
    "# Validate that datasets are not empty\n",
    "# - Raise an error if no valid images are found, directing to the log file\n",
    "if len(train_dataset) == 0:\n",
    "    raise ValueError(\"Train dataset is empty. Check /home/ghulam/FF++/invalid_images.txt for details.\")\n",
    "if len(val_dataset) == 0:\n",
    "    raise ValueError(\"Validation dataset is empty. Check /home/ghulam/FF++/invalid_images.txt for details.\")\n",
    "if len(test_dataset) == 0:\n",
    "    raise ValueError(\"Test dataset is empty. Check /home/ghulam/FF++/invalid_images.txt for details.\")\n",
    "\n",
    "# Define a custom collate function to handle None entries in batches\n",
    "def collate_fn(batch):\n",
    "    # Filter out None items (failed image loads)\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    # Return None if the batch is empty\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    # Use default collate to combine valid items into tensors\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "# Create DataLoader for training dataset\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4,  # Process 4 images per batch\n",
    "    shuffle=True,  # Shuffle data for training to improve generalization\n",
    "    num_workers=4,  # Use 4 CPU workers for parallel data loading\n",
    "    pin_memory=True,  # Enable pinned memory for faster data transfer to GPU\n",
    "    collate_fn=collate_fn  # Use custom collate function to handle None\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation dataset\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=4,  # Process 4 images per batch\n",
    "    shuffle=False,  # No shuffling to maintain order for evaluation\n",
    "    num_workers=4,  # Use 4 CPU workers for parallel data loading\n",
    "    pin_memory=True,  # Enable pinned memory for faster data transfer to GPU\n",
    "    collate_fn=collate_fn  # Use custom collate function to handle None\n",
    ")\n",
    "\n",
    "# Create DataLoader for test dataset\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=4,  # Process 4 images per batch\n",
    "    shuffle=False,  # No shuffling to maintain order for evaluation\n",
    "    num_workers=4,  # Use 4 CPU workers for parallel data loading\n",
    "    pin_memory=True,  # Enable pinned memory for faster data transfer to GPU\n",
    "    collate_fn=collate_fn  # Use custom collate function to handle None\n",
    ")\n",
    "\n",
    "# Print the number of batches in each DataLoader\n",
    "print(f\"Train loader batches: {len(train_loader)}, Val loader batches: {len(val_loader)}, Test loader batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombinedModel initialized for single-GPU training.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for PyTorch, neural networks, functional operations, EfficientNet, and Vision Transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from transformers import ViTConfig, ViTForImageClassification\n",
    "\n",
    "# Define a custom model combining EfficientNet and Vision Transformer (ViT)\n",
    "class CombinedModel(nn.Module):\n",
    "    # Initialize with pre-trained EfficientNet and ViT models\n",
    "    def __init__(self, efficientnet, vit):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        # Extract EfficientNet layers, excluding the last two (avgpool and classifier)\n",
    "        self.efficientnet = nn.Sequential(*list(efficientnet.children())[:-2])\n",
    "        # Store ViT model\n",
    "        self.vit = vit\n",
    "        # Add adaptive average pooling to reduce EfficientNet features to 1x1\n",
    "        self.efficientnet_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        # Define feature dimensions: EfficientNet (1536), ViT (from config)\n",
    "        eff_features_dim = 1536\n",
    "        vit_features_dim = vit.config.hidden_size\n",
    "        # Create a fully connected layer to combine features and output 2 classes (real/fake)\n",
    "        self.fc = nn.Linear(eff_features_dim + vit_features_dim, 2)\n",
    "\n",
    "    # Define forward pass for input images\n",
    "    def forward(self, x):\n",
    "        # Extract features from EfficientNet\n",
    "        eff_features = self.efficientnet(x)\n",
    "        # Apply average pooling to reduce spatial dimensions\n",
    "        eff_features = self.efficientnet_avgpool(eff_features)\n",
    "        # Flatten features to 1D\n",
    "        eff_features = eff_features.view(eff_features.size(0), -1)\n",
    "        # Resize input for ViT to ensure 224x224 resolution\n",
    "        vit_input = F.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "        # Pass through ViT, retrieving hidden states\n",
    "        vit_outputs = self.vit(vit_input, output_hidden_states=True, return_dict=True)\n",
    "        # Extract CLS token features from the last hidden state\n",
    "        vit_features = vit_outputs.hidden_states[-1][:, 0]\n",
    "        # Concatenate EfficientNet and ViT features\n",
    "        combined_features = torch.cat((eff_features, vit_features), dim=1)\n",
    "        # Pass combined features through fully connected layer for classification\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Set device to GPU (cuda:1) for single-GPU training\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "# Load pre-trained EfficientNet-B3 model with ImageNet weights\n",
    "efficientnet = models.efficientnet_b3(weights=\"EfficientNet_B3_Weights.IMAGENET1K_V1\")\n",
    "# Modify the classifier to output 2 classes (real/fake)\n",
    "efficientnet.classifier[1] = nn.Linear(efficientnet.classifier[1].in_features, 2)\n",
    "# Move EfficientNet model to the specified GPU\n",
    "efficientnet.to(device)\n",
    "\n",
    "# Load configuration for Vision Transformer (ViT) with 2 output labels\n",
    "config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "config.num_labels = 2\n",
    "# Initialize ViT model with pre-trained weights, ignoring mismatched classifier sizes\n",
    "vit = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "# Replace ViT classifier with a new layer for 2 classes\n",
    "vit.classifier = nn.Linear(vit.config.hidden_size, 2)\n",
    "# Move ViT model to the specified GPU\n",
    "vit.to(device)\n",
    "\n",
    "# Create an instance of the combined model with EfficientNet and ViT\n",
    "combined_model = CombinedModel(efficientnet, vit)\n",
    "# Move the combined model to the specified GPU\n",
    "combined_model.to(device)\n",
    "\n",
    "# Define the loss function (cross-entropy for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the Adam optimizer with a learning rate of 1e-4 for model parameters\n",
    "optimizer = torch.optim.Adam(combined_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Confirm that the model is initialized and ready for training on a single GPU\n",
    "print(\"CombinedModel initialized for single-GPU training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function with early stopping defined.\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch AMP modules for mixed precision training\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# Define a function to train the combined model with early stopping\n",
    "def train_combined_model(model, train_loader, val_loader, criterion, optimizer, epochs, device, patience=2):\n",
    "    # Move the model to the specified device (GPU)\n",
    "    model.to(device)\n",
    "    # Initialize gradient scaler for mixed precision training on CUDA\n",
    "    scaler = GradScaler('cuda')\n",
    "    # Initialize best validation loss to infinity\n",
    "    best_val_loss = float('inf')\n",
    "    # Counter for epochs without improvement\n",
    "    epochs_no_improve = 0\n",
    "    # Variable to store the best model state\n",
    "    best_model_state = None\n",
    "\n",
    "    # Loop over the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Set model to training mode (enables dropout and batch normalization updates)\n",
    "        model.train()\n",
    "        # Initialize metrics for training\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        # Iterate over batches in the training DataLoader\n",
    "        for images, labels in train_loader:\n",
    "            # Skip batches with None (invalid images)\n",
    "            if images is None:\n",
    "                continue\n",
    "            # Move images and labels to the specified device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Clear gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Enable mixed precision for forward pass\n",
    "            with autocast('cuda'):\n",
    "                # Get model predictions\n",
    "                outputs = model(images)\n",
    "                # Compute loss using the provided criterion\n",
    "                loss = criterion(outputs, labels)\n",
    "            # Scale loss and perform backpropagation\n",
    "            scaler.scale(loss).backward()\n",
    "            # Update model parameters using scaled gradients\n",
    "            scaler.step(optimizer)\n",
    "            # Update the scaler for the next iteration\n",
    "            scaler.update()\n",
    "            # Accumulate training loss (weighted by batch size)\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            # Count correct predictions\n",
    "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            # Track total samples processed\n",
    "            train_total += labels.size(0)\n",
    "        # Compute average training loss\n",
    "        train_loss /= train_total\n",
    "        # Compute training accuracy\n",
    "        train_accuracy = train_correct / train_total\n",
    "\n",
    "        # Set model to evaluation mode (disables dropout and batch normalization updates)\n",
    "        model.eval()\n",
    "        # Initialize metrics for validation\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        # Disable gradient computation for validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over batches in the validation DataLoader\n",
    "            for images, labels in val_loader:\n",
    "                # Skip batches with None (invalid images)\n",
    "                if images is None:\n",
    "                    continue\n",
    "                # Move images and labels to the specified device\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                # Enable mixed precision for forward pass\n",
    "                with autocast('cuda'):\n",
    "                    # Get model predictions\n",
    "                    outputs = model(images)\n",
    "                    # Compute loss\n",
    "                    loss = criterion(outputs, labels)\n",
    "                # Accumulate validation loss (weighted by batch size)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                # Count correct predictions\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                # Track total samples processed\n",
    "                val_total += labels.size(0)\n",
    "        # Compute average validation loss\n",
    "        val_loss /= val_total\n",
    "        # Compute validation accuracy\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # Print training and validation metrics for the current epoch\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2%}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2%}\")\n",
    "\n",
    "        # Check if validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            # Update best validation loss\n",
    "            best_val_loss = val_loss\n",
    "            # Save the current model state\n",
    "            best_model_state = model.state_dict()\n",
    "            # Reset counter for epochs without improvement\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            # Increment counter for epochs without improvement\n",
    "            epochs_no_improve += 1\n",
    "            # Check if early stopping criterion is met\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                # Restore the best model state\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n",
    "# Confirm that the training function with early stopping is defined\n",
    "print(\"Training function with early stopping defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68996/4039599809.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68996/4039599809.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipykernel_68996/4039599809.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.1216, Train Acc: 95.58%, Val Loss: 0.0174, Val Acc: 99.40%\n",
      "Epoch 2/3, Train Loss: 0.0436, Train Acc: 98.52%, Val Loss: 0.0159, Val Acc: 99.47%\n",
      "Epoch 3/3, Train Loss: 0.0324, Train Acc: 99.09%, Val Loss: 0.0197, Val Acc: 99.44%\n",
      "Free GPU memory: 9.92 GB\n",
      "Total GPU memory: 14.58 GB\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch for GPU memory management\n",
    "import torch\n",
    "\n",
    "# Clear GPU memory to free up resources before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train the CombinedModel for 3 epochs\n",
    "# - Pass the model, data loaders, loss function, optimizer, number of epochs, and device\n",
    "combined_model = train_combined_model(\n",
    "    model=combined_model,  # Pre-initialized CombinedModel (EfficientNet + ViT)\n",
    "    train_loader=train_loader,  # DataLoader for training dataset\n",
    "    val_loader=val_loader,  # DataLoader for validation dataset\n",
    "    criterion=criterion,  # Cross-entropy loss function\n",
    "    optimizer=optimizer,  # Adam optimizer with learning rate\n",
    "    epochs=3,  # Train for 3 epochs\n",
    "    device=device  # GPU device (cuda:1)\n",
    ")\n",
    "\n",
    "# Check GPU memory usage after training\n",
    "# - Retrieve free and total memory in bytes\n",
    "free, total = torch.cuda.mem_get_info()\n",
    "# Convert free memory to gigabytes and print with 2 decimal places\n",
    "print(f\"Free GPU memory: {free / 1024**3:.2f} GB\")\n",
    "# Convert total memory to gigabytes and print with 2 decimal places\n",
    "print(f\"Total GPU memory: {total / 1024**3:.2f} GB\")\n",
    "\n",
    "# Confirm that training has completed\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/ghulam/FF++/combined_model_epoch3.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(combined_model.state_dict(), \"/home/ghulam/FF++/combined_model_epoch3.pth\")\n",
    "print(\"Model saved to /home/ghulam/FF++/combined_model_epoch3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1483, Test Acc: 96.33%\n"
     ]
    }
   ],
   "source": [
    "# Define a function to evaluate the model on the test dataset\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    # Set the model to evaluation mode (disables dropout and batch normalization updates)\n",
    "    model.eval()\n",
    "    # Initialize variables to track loss, correct predictions, and total samples\n",
    "    test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "    # Disable gradient computation for efficiency during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the test DataLoader\n",
    "        for images, labels in test_loader:\n",
    "            # Skip batches with None (invalid images)\n",
    "            if images is None:\n",
    "                continue\n",
    "            # Move images and labels to the specified device (GPU)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Enable mixed precision for forward pass on CUDA\n",
    "            with autocast('cuda'):\n",
    "                # Get model predictions\n",
    "                outputs = model(images)\n",
    "                # Compute loss using the provided criterion\n",
    "                loss = criterion(outputs, labels)\n",
    "            # Accumulate test loss (weighted by batch size)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            # Count correct predictions by comparing predicted and true labels\n",
    "            test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            # Track total number of samples processed\n",
    "            test_total += labels.size(0)\n",
    "    # Compute average test loss\n",
    "    test_loss /= test_total\n",
    "    # Compute test accuracy as the fraction of correct predictions\n",
    "    test_accuracy = test_correct / test_total\n",
    "    # Print formatted test loss and accuracy\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2%}\")\n",
    "    # Return computed metrics\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "# Evaluate the combined model on the FF++ test dataset\n",
    "# - Pass the trained model, test DataLoader, loss function, and device\n",
    "test_loss, test_accuracy = evaluate_model(combined_model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.0201, Train Acc: 99.36%, Val Loss: 0.0147, Val Acc: 99.58%\n",
      "Epoch 2/5, Train Loss: 0.0202, Train Acc: 99.41%, Val Loss: 0.0065, Val Acc: 99.84%\n",
      "Epoch 3/5, Train Loss: 0.0131, Train Acc: 99.65%, Val Loss: 0.0176, Val Acc: 99.54%\n",
      "Epoch 4/5, Train Loss: 0.0123, Train Acc: 99.62%, Val Loss: 0.0060, Val Acc: 99.84%\n",
      "Epoch 5/5, Train Loss: 0.0104, Train Acc: 99.72%, Val Loss: 0.0093, Val Acc: 99.68%\n",
      "Free GPU memory: 9.92 GB\n",
      "Total GPU memory: 14.58 GB\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Train with 5 epochs\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "combined_model = train_combined_model(\n",
    "    model=combined_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    epochs=5,\n",
    "    device=device,\n",
    "    patience=2\n",
    ")\n",
    "free, total = torch.cuda.mem_get_info()\n",
    "print(f\"Free GPU memory: {free / 1024**3:.2f} GB\")\n",
    "print(f\"Total GPU memory: {total / 1024**3:.2f} GB\")\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/ghulam/FF++/combined_model_epoch5.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(combined_model.state_dict(), \"/home/ghulam/FF++/combined_model_epoch5.pth\")\n",
    "print(\"Model saved to /home/ghulam/FF++/combined_model_epoch5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0862, Test Acc: 98.00%\n"
     ]
    }
   ],
   "source": [
    "# Test Set Evaluation 2\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            if images is None:\n",
    "                continue\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            with autocast('cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "    test_loss /= test_total\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2%}\")\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "test_loss, test_accuracy = evaluate_model(combined_model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 4\n",
      "GPU name: Tesla T4\n",
      "CUDA version: 12.1\n",
      "CuDNN version: 90100\n"
     ]
    }
   ],
   "source": [
    "# # Verify GPU availability and configuration\n",
    "# import torch\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "#     print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(f\"CUDA version: {torch.version.cuda}\")\n",
    "#     print(f\"CuDNN version: {torch.backends.cudnn.version()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "losh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
