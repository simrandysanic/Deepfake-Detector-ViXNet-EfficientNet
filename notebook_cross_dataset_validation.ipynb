{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cfe021e",
   "metadata": {},
   "source": [
    "### Cross-Dataset Validation on Celeb-DF Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b858f0",
   "metadata": {},
   "source": [
    "> **NOTE:**  \n",
    "> Activate the virtual environment before running:  \n",
    "> `conda activate new_env`\n",
    ">\n",
    "> If the environment does not exist, create it using the following commands:\n",
    ">\n",
    "> ```\n",
    "> conda create -n new_env python=3.12 -y\n",
    "> conda activate new_env\n",
    "> ```\n",
    ">\n",
    "> **Select the interpreter from the upper right-hand side (RHS) in Jupyter:**  \n",
    "> `new_env (Python 3.12.9) miniconda3/envs/new_env/bin/python - Conda Env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for face detection, file handling, and image processing\n",
    "from facenet_pytorch import MTCNN\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize MTCNN model for face detection\n",
    "# - image_size=224: Resize detected faces to 224x224 pixels\n",
    "# - margin=20: Add 20-pixel margin around detected faces\n",
    "# - device='cpu': Run on CPU to avoid CUDA issues\n",
    "# - keep_all=False: Return only the most confident face per image\n",
    "mtcnn = MTCNN(image_size=224, margin=20, device='cpu', keep_all=False)\n",
    "\n",
    "# Define input and output directories\n",
    "# - input_dir: Source directory containing raw Celeb-DF images in 'real' and 'fake' subfolders\n",
    "# - output_dir: Destination directory to save cropped face images\n",
    "input_dir = \"/home/ghulam/Celeb-DF/output_folder\"\n",
    "output_dir = \"/home/ghulam/Celeb-DF/test\"\n",
    "\n",
    "# Iterate over 'real' and 'fake' labels\n",
    "for label in ['real', 'fake']:\n",
    "    # Loop through subdirectories (e.g., video folders) in the current label folder\n",
    "    for subdir in os.listdir(os.path.join(input_dir, label)):\n",
    "        # Construct full paths for input and output directories\n",
    "        in_path = os.path.join(input_dir, label, subdir)\n",
    "        out_path = os.path.join(output_dir, label, subdir)\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(out_path, exist_ok=True)\n",
    "        \n",
    "        # Process each image in the subdirectory\n",
    "        for img_name in os.listdir(in_path):\n",
    "            # Check if the file is an image (JPG or PNG)\n",
    "            if img_name.endswith(('.jpg', '.png')):\n",
    "                # Open the image using PIL\n",
    "                img = Image.open(os.path.join(in_path, img_name))\n",
    "                \n",
    "                # Detect and crop the face using MTCNN; returns a single face tensor or None\n",
    "                face = mtcnn(img)\n",
    "                \n",
    "                # If a face is detected, process and save it\n",
    "                if face is not None:\n",
    "                    # Convert face tensor: permute to (H,W,C), scale to 0-255, convert to numpy\n",
    "                    face = face.permute(1, 2, 0).mul(255).byte().numpy()\n",
    "                    # Save the cropped face as an image in the output directory\n",
    "                    Image.fromarray(face).save(os.path.join(out_path, img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97cfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for PyTorch, neural networks, EfficientNet, and Vision Transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from transformers import ViTConfig, ViTForImageClassification\n",
    "\n",
    "# Define a custom model combining EfficientNet and Vision Transformer (ViT)\n",
    "class CombinedModel(nn.Module):\n",
    "    # Initialize with pre-trained EfficientNet and ViT models\n",
    "    def __init__(self, efficientnet, vit):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        # Extract EfficientNet layers, excluding the last two (avgpool and classifier)\n",
    "        self.efficientnet = nn.Sequential(*list(efficientnet.children())[:-2])\n",
    "        # Store ViT model\n",
    "        self.vit = vit\n",
    "        # Add adaptive average pooling to reduce EfficientNet features to 1x1\n",
    "        self.efficientnet_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        # Define feature dimensions: EfficientNet (1536), ViT (from config)\n",
    "        eff_features_dim = 1536\n",
    "        vit_features_dim = vit.config.hidden_size\n",
    "        # Create a fully connected layer to combine features and output 2 classes (real/fake)\n",
    "        self.fc = nn.Linear(eff_features_dim + vit_features_dim, 2)\n",
    "\n",
    "    # Define forward pass for input images\n",
    "    def forward(self, x):\n",
    "        # Extract features from EfficientNet\n",
    "        eff_features = self.efficientnet(x)\n",
    "        # Apply average pooling to reduce spatial dimensions\n",
    "        eff_features = self.efficientnet_avgpool(eff_features)\n",
    "        # Flatten features to 1D\n",
    "        eff_features = eff_features.view(eff_features.size(0), -1)\n",
    "        # Resize input for ViT (ensure 224x224)\n",
    "        vit_input = torch.nn.functional.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "        # Pass through ViT, retrieving hidden states\n",
    "        vit_outputs = self.vit(vit_input, output_hidden_states=True, return_dict=True)\n",
    "        # Extract CLS token features from the last hidden state\n",
    "        vit_features = vit_outputs.hidden_states[-1][:, 0]\n",
    "        # Concatenate EfficientNet and ViT features\n",
    "        combined_features = torch.cat((eff_features, vit_features), dim=1)\n",
    "        # Pass combined features through fully connected layer for classification\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Set device to CPU to avoid CUDA issues\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load pre-trained EfficientNet-B3 model with ImageNet weights\n",
    "efficientnet = models.efficientnet_b3(weights=\"EfficientNet_B3_Weights.IMAGENET1K_V1\")\n",
    "# Modify classifier to output 2 classes (real/fake)\n",
    "efficientnet.classifier[1] = nn.Linear(efficientnet.classifier[1].in_features, 2)\n",
    "# Move EfficientNet to CPU\n",
    "efficientnet.to(device)\n",
    "\n",
    "# Configure ViT model with 2 output labels\n",
    "config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "config.num_labels = 2\n",
    "# Load pre-trained ViT model, ignoring mismatched classifier weights\n",
    "vit = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "# Replace ViT classifier with a new layer for 2 classes\n",
    "vit.classifier = nn.Linear(vit.config.hidden_size, 2)\n",
    "# Move ViT to CPU\n",
    "vit.to(device)\n",
    "\n",
    "# Create combined model instance with EfficientNet and ViT\n",
    "combined_model = CombinedModel(efficientnet, vit)\n",
    "# Move combined model to CPU\n",
    "combined_model.to(device)\n",
    "\n",
    "# Load pre-trained weights from FF++ training, mapping to CPU\n",
    "combined_model.load_state_dict(torch.load(\"/home/ghulam/FF++/combined_model_epoch5.pth\", map_location=torch.device('cpu')))\n",
    "# Set model to evaluation mode (disable dropout, batch norm updates)\n",
    "combined_model.eval()\n",
    "\n",
    "# Confirm successful model loading\n",
    "print(\"Model loaded successfully on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15457 valid images out of 15457\n",
      "Cross-dataset test samples: 15457, Batches: 3865\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for file handling, dataset creation, and image processing\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define image transformations to match FF++ preprocessing\n",
    "transform = transforms.Compose([\n",
    "    # Resize images to 224x224 pixels\n",
    "    transforms.Resize((224, 224)),\n",
    "    # Convert images to PyTorch tensors\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize pixel values using ImageNet mean and standard deviation\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom dataset class for Celeb-DF images\n",
    "class DeepfakeDataset(Dataset):\n",
    "    # Initialize with image paths, labels, and transformation pipeline\n",
    "    def __init__(self, image_paths, labels, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.valid_indices = []\n",
    "        # Create a log file to track invalid images\n",
    "        log_file = \"/home/ghulam/Celeb-DF/invalid_images.txt\"\n",
    "        with open(log_file, \"w\") as log:\n",
    "            # Check each image for validity\n",
    "            for idx, path in enumerate(self.image_paths):\n",
    "                # Skip if file doesn't exist\n",
    "                if not os.path.isfile(path):\n",
    "                    print(f\"Skipping missing file: {path}\")\n",
    "                    log.write(f\"{path}: File does not exist\\n\")\n",
    "                    continue\n",
    "                try:\n",
    "                    # Attempt to open and validate image as RGB\n",
    "                    img = Image.open(path).convert(\"RGB\")\n",
    "                    img.close()\n",
    "                    self.valid_indices.append(idx)\n",
    "                except Exception as e:\n",
    "                    # Log and skip invalid images\n",
    "                    print(f\"Skipping invalid image: {path} ({e})\")\n",
    "                    log.write(f\"{path}: {e}\\n\")\n",
    "        # Report number of valid images loaded\n",
    "        print(f\"Loaded {len(self.valid_indices)} valid images out of {len(self.image_paths)}\")\n",
    "\n",
    "    # Return number of valid images in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    # Retrieve image and label for a given index\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        try:\n",
    "            # Load and convert image to RGB\n",
    "            image = Image.open(self.image_paths[actual_idx]).convert(\"RGB\")\n",
    "            # Apply transformations (resize, normalize, etc.)\n",
    "            image = self.transform(image)\n",
    "            # Convert label to PyTorch tensor\n",
    "            label = torch.tensor(self.labels[actual_idx], dtype=torch.long)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            # Log errors during loading and return None\n",
    "            print(f\"Error loading image {self.image_paths[actual_idx]}: {e}\")\n",
    "            return None\n",
    "\n",
    "# Custom collate function to handle None entries in batch\n",
    "def collate_fn(batch):\n",
    "    # Filter out None items (failed image loads)\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    # Return None if batch is empty\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    # Use default collate to combine valid items into tensors\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "# Define base directory for Celeb-DF dataset\n",
    "base_dir = \"/home/ghulam/Celeb-DF\"\n",
    "# Set test directory containing preprocessed images\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "# Collect paths for real images from all subdirectories\n",
    "real_test_images = [os.path.join(test_dir, \"real\", subdir, img) \n",
    "                    for subdir in os.listdir(os.path.join(test_dir, \"real\")) \n",
    "                    for img in os.listdir(os.path.join(test_dir, \"real\", subdir))]\n",
    "# Collect paths for fake images from all subdirectories\n",
    "fake_test_images = [os.path.join(test_dir, \"fake\", subdir, img) \n",
    "                    for subdir in os.listdir(os.path.join(test_dir, \"fake\")) \n",
    "                    for img in os.listdir(os.path.join(test_dir, \"fake\", subdir))]\n",
    "# Combine real and fake image paths\n",
    "test_paths = real_test_images + fake_test_images\n",
    "# Assign labels: 0 for real, 1 for fake\n",
    "test_labels = [0] * len(real_test_images) + [1] * len(fake_test_images)\n",
    "\n",
    "# Create dataset instance for Celeb-DF test set\n",
    "cross_test_dataset = DeepfakeDataset(test_paths, test_labels, transform)\n",
    "# Create DataLoader for batch processing\n",
    "cross_test_loader = DataLoader(\n",
    "    cross_test_dataset, \n",
    "    batch_size=4,  # Process 4 images per batch\n",
    "    shuffle=False,  # Maintain order for evaluation\n",
    "    num_workers=4,  # Use 4 CPU workers for loading\n",
    "    pin_memory=False,  # Disable pinned memory for CPU\n",
    "    collate_fn=collate_fn  # Use custom collate function\n",
    ")\n",
    "# Print total samples and number of batches\n",
    "print(f\"Cross-dataset test samples: {len(test_paths)}, Batches: {len(cross_test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for PyTorch, neural networks, EfficientNet, and Vision Transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from transformers import ViTConfig, ViTForImageClassification\n",
    "\n",
    "# Define a custom model combining EfficientNet and Vision Transformer (ViT)\n",
    "class CombinedModel(nn.Module):\n",
    "    # Initialize with pre-trained EfficientNet and ViT models\n",
    "    def __init__(self, efficientnet, vit):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        # Extract EfficientNet layers, excluding the last two (avgpool and classifier)\n",
    "        self.efficientnet = nn.Sequential(*list(efficientnet.children())[:-2])\n",
    "        # Store ViT model\n",
    "        self.vit = vit\n",
    "        # Add adaptive average pooling to reduce EfficientNet features to 1x1\n",
    "        self.efficientnet_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        # Define feature dimensions: EfficientNet (1536), ViT (from config)\n",
    "        eff_features_dim = 1536\n",
    "        vit_features_dim = vit.config.hidden_size\n",
    "        # Create a fully connected layer to combine features and output 2 classes (real/fake)\n",
    "        self.fc = nn.Linear(eff_features_dim + vit_features_dim, 2)\n",
    "\n",
    "    # Define forward pass for input images\n",
    "    def forward(self, x):\n",
    "        # Extract features from EfficientNet\n",
    "        eff_features = self.efficientnet(x)\n",
    "        # Apply average pooling to reduce spatial dimensions\n",
    "        eff_features = self.efficientnet_avgpool(eff_features)\n",
    "        # Flatten features to 1D\n",
    "        eff_features = eff_features.view(eff_features.size(0), -1)\n",
    "        # Resize input for ViT to ensure 224x224 resolution\n",
    "        vit_input = torch.nn.functional.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "        # Pass through ViT, retrieving hidden states\n",
    "        vit_outputs = self.vit(vit_input, output_hidden_states=True, return_dict=True)\n",
    "        # Extract CLS token features from the last hidden state\n",
    "        vit_features = vit_outputs.hidden_states[-1][:, 0]\n",
    "        # Concatenate EfficientNet and ViT features\n",
    "        combined_features = torch.cat((eff_features, vit_features), dim=1)\n",
    "        # Pass combined features through fully connected layer for classification\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Set device to CPU to avoid CUDA compatibility issues\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load pre-trained EfficientNet-B3 model with ImageNet weights\n",
    "efficientnet = models.efficientnet_b3(weights=\"EfficientNet_B3_Weights.IMAGENET1K_V1\")\n",
    "# Modify the classifier to output 2 classes (real/fake)\n",
    "efficientnet.classifier[1] = nn.Linear(efficientnet.classifier[1].in_features, 2)\n",
    "# Move EfficientNet model to CPU\n",
    "efficientnet.to(device)\n",
    "\n",
    "# Load configuration for Vision Transformer (ViT) with 2 output labels\n",
    "config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "config.num_labels = 2\n",
    "# Initialize ViT model with pre-trained weights, ignoring mismatched classifier sizes\n",
    "vit = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "# Replace ViT classifier with a new layer for 2 classes\n",
    "vit.classifier = nn.Linear(vit.config.hidden_size, 2)\n",
    "# Move ViT model to CPU\n",
    "vit.to(device)\n",
    "\n",
    "# Create an instance of the combined model with EfficientNet and ViT\n",
    "combined_model = CombinedModel(efficientnet, vit)\n",
    "# Move the combined model to CPU\n",
    "combined_model.to(device)\n",
    "\n",
    "# Load pre-trained weights from FF++ training, mapping to CPU to avoid CUDA issues\n",
    "combined_model.load_state_dict(torch.load(\"/home/ghulam/FF++/combined_model_epoch5.pth\", map_location=torch.device('cpu')))\n",
    "# Set the model to evaluation mode (disables dropout and batch normalization updates)\n",
    "combined_model.eval()\n",
    "\n",
    "# Confirm that the model was loaded successfully\n",
    "print(\"Model loaded successfully on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917c828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3248, Test Acc: 50.42%\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch's neural network module for loss calculation\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a function to evaluate the model on a test dataset\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    # Set the model to evaluation mode (disables dropout and batch normalization updates)\n",
    "    model.eval()\n",
    "    # Initialize variables to track loss, correct predictions, and total samples\n",
    "    test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "    # Disable gradient computation for efficiency during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the test DataLoader\n",
    "        for images, labels in test_loader:\n",
    "            # Skip batches with None (invalid images)\n",
    "            if images is None:\n",
    "                continue\n",
    "            # Move images and labels to the specified device (CPU)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Forward pass: get model predictions\n",
    "            outputs = model(images)\n",
    "            # Compute loss using the provided criterion\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Accumulate loss (weighted by batch size)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            # Count correct predictions by comparing predicted and true labels\n",
    "            test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            # Track total number of samples processed\n",
    "            test_total += labels.size(0)\n",
    "    # Compute average test loss\n",
    "    test_loss /= test_total\n",
    "    # Compute test accuracy as the fraction of correct predictions\n",
    "    test_accuracy = test_correct / test_total\n",
    "    # Print formatted test loss and accuracy\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2%}\")\n",
    "    # Return computed metrics\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "# Define the loss function (cross-entropy for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Evaluate the combined model on the Celeb-DF test dataset\n",
    "test_loss, test_accuracy = evaluate_model(combined_model, cross_test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03927147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[TN=2473 FP=2851]\n",
      " [FN=4812 TP=5321]]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for PyTorch, confusion matrix calculation, and numerical operations\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to compute the confusion matrix for model predictions\n",
    "def get_confusion_matrix(model, test_loader, device):\n",
    "    # Set the model to evaluation mode (disables dropout and batch normalization updates)\n",
    "    model.eval()\n",
    "    # Initialize lists to store all predictions and true labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    # Disable gradient computation for efficiency during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the test DataLoader\n",
    "        for images, labels in test_loader:\n",
    "            # Skip batches with None (invalid images)\n",
    "            if images is None:\n",
    "                continue\n",
    "            # Move images and labels to the specified device (CPU)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Forward pass: get model predictions\n",
    "            outputs = model(images)\n",
    "            # Get predicted class indices by selecting the highest output score\n",
    "            preds = outputs.argmax(1)\n",
    "            # Convert predictions to numpy and append to the list\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            # Convert true labels to numpy and append to the list\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    # Compute the confusion matrix using true labels and predictions\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    # Print the confusion matrix in a readable format\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"[[TN={cm[0,0]} FP={cm[0,1]}]\")  # True Negatives, False Positives (Real class)\n",
    "    print(f\" [FN={cm[1,0]} TP={cm[1,1]}]]\")  # False Negatives, True Positives (Fake class)\n",
    "    # Return the confusion matrix\n",
    "    return cm\n",
    "\n",
    "# Compute and display the confusion matrix for the combined model on Celeb-DF test data\n",
    "cm = get_confusion_matrix(combined_model, cross_test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f1566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.34      0.46      0.39      5324\n",
      "        Fake       0.65      0.53      0.58     10133\n",
      "\n",
      "    accuracy                           0.50     15457\n",
      "   macro avg       0.50      0.49      0.49     15457\n",
      "weighted avg       0.54      0.50      0.52     15457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import library for generating classification metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define a function to compute classification metrics (precision, recall, F1-score)\n",
    "def get_classification_metrics(model, test_loader, device):\n",
    "    # Set the model to evaluation mode (disables dropout and batch normalization updates)\n",
    "    model.eval()\n",
    "    # Initialize lists to store all predictions and true labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    # Disable gradient computation for efficiency during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the test DataLoader\n",
    "        for images, labels in test_loader:\n",
    "            # Skip batches with None (invalid images)\n",
    "            if images is None:\n",
    "                continue\n",
    "            # Move images and labels to the specified device (CPU)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Forward pass: get model predictions\n",
    "            outputs = model(images)\n",
    "            # Get predicted class indices by selecting the highest output score\n",
    "            preds = outputs.argmax(1)\n",
    "            # Convert predictions to numpy and append to the list\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            # Convert true labels to numpy and append to the list\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    # Print a detailed classification report with precision, recall, and F1-score for Real and Fake classes\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))\n",
    "    # Return the classification report as a dictionary for further use\n",
    "    return classification_report(all_labels, all_preds, output_dict=True)\n",
    "\n",
    "# Compute and display classification metrics for the combined model on Celeb-DF test data\n",
    "metrics = get_classification_metrics(combined_model, cross_test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16106a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Accuracy: 46.45%\n",
      "Fake Accuracy: 52.51%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy for the Real class using confusion matrix\n",
    "# - cm[0,0]: True Negatives (correctly predicted Real)\n",
    "# - cm[0,1]: False Positives (Real predicted as Fake)\n",
    "# - Compute accuracy as TN / (TN + FP), return 0 if denominator is 0\n",
    "real_acc = cm[0,0] / (cm[0,0] + cm[0,1]) if (cm[0,0] + cm[0,1]) > 0 else 0\n",
    "\n",
    "# Calculate accuracy for the Fake class using confusion matrix\n",
    "# - cm[1,1]: True Positives (correctly predicted Fake)\n",
    "# - cm[1,0]: False Negatives (Fake predicted as Real)\n",
    "# - Compute accuracy as TP / (TP + FN), return 0 if denominator is 0\n",
    "fake_acc = cm[1,1] / (cm[1,1] + cm[1,0]) if (cm[1,1] + cm[1,0]) > 0 else 0\n",
    "\n",
    "# Print Real class accuracy formatted as a percentage with 2 decimal places\n",
    "print(f\"Real Accuracy: {real_acc:.2%}\")\n",
    "\n",
    "# Print Fake class accuracy formatted as a percentage with 2 decimal places\n",
    "print(f\"Fake Accuracy: {fake_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417f4f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/ghulam/Celeb-DF/evaluation_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/ghulam/Celeb-DF/evaluation_results.txt\", \"w\") as f:\n",
    "    f.write(f\"Test Loss: 1.3248\\n\")\n",
    "    f.write(f\"Test Acc: 50.42%\\n\")\n",
    "    f.write(f\"Confusion Matrix:\\n[[TN=2473 FP=2851]\\n [FN=4812 TP=5321]]\\n\")\n",
    "    f.write(f\"Real Accuracy: 46.45%\\n\")\n",
    "    f.write(f\"Fake Accuracy: 52.51%\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(\"              precision    recall  f1-score   support\\n\\n\")\n",
    "    f.write(\"       Real       0.34      0.46      0.39      5324\\n\")\n",
    "    f.write(\"       Fake       0.65      0.53      0.58     10133\\n\\n\")\n",
    "    f.write(\"    accuracy                           0.50     15457\\n\")\n",
    "    f.write(\"   macro avg       0.50      0.49      0.49     15457\\n\")\n",
    "    f.write(\"weighted avg       0.54      0.50      0.52     15457\\n\")\n",
    "print(\"Results saved to /home/ghulam/Celeb-DF/evaluation_results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
